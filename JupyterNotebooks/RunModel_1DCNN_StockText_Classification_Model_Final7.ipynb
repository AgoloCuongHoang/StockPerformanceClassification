{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "C:\\Users\\pattyry\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################################################\n",
    "#\n",
    "# Stock future performance classification based on text\n",
    "#\n",
    "# Approach:\n",
    "#\n",
    "# Build on top of it a 1D convolutional neural network, ending in a softmax output over 3 even categories.\n",
    "# Use word Glove word vectors for large English text corpus as inputs model\n",
    "#\n",
    "# Steps\n",
    "# 1) After cleaning, we convert all text samples in the dataset into sequences of word indices.  In this case, a \"word index\" would simply be an integer ID for the word. \n",
    "# 2) We consider the top 350,000 most commonly occuring words in the dataset\n",
    "# 3) We truncate the sequences to a maximum length of 25,000 words.\n",
    "# 5) We [repare an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "# 6) Then, we load this embedding matrix into a Keras Embedding layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "#\n",
    "###############################################################################################################################################\n",
    "\n",
    "# import libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import zip\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas import DataFrame   \n",
    "import pickle\n",
    "import re\n",
    "import sys \n",
    "import azureml\n",
    "import string\n",
    "from scipy import stats\n",
    "import pip\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer     \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Input, Flatten \n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding \n",
    "from keras.models import Model \n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import initializers \n",
    "from keras.layers import regularizers \n",
    "from keras.layers import constraints \n",
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.constraints import max_norm\n",
    "import keras.backend as K\n",
    "import os\n",
    "import tempfile  \n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import Phrases, phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec as wv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from IPython.display import SVG\n",
    "import cloudpickle\n",
    "import csv\n",
    "import mkl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import io\n",
    "from os.path import dirname, join\n",
    "import regex\n",
    "import graphviz\n",
    "import pydotplus\n",
    "import pyparsing\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data unpickled\n",
      "Review the unique labels [0, 2, 1]\n",
      "Categories (3, int64): [0 < 1 < 2]\n",
      "(916, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>Return3Bin_4Weeks</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>72955</td>\n",
       "      <td>0</td>\n",
       "      <td>acad overview we are biopharmaceutical company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>74741</td>\n",
       "      <td>2</td>\n",
       "      <td>acad company overview we are biopharmaceutical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count Return3Bin_4Weeks  \\\n",
       "131  72955                 0   \n",
       "133  74741                 2   \n",
       "\n",
       "                                             CleanText  \n",
       "131  acad overview we are biopharmaceutical company...  \n",
       "133  acad company overview we are biopharmaceutical...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##########################################\n",
    "# Get Previously Organized Stock Data\n",
    "##########################################\n",
    "\n",
    "os.chdir('C:\\\\users\\\\pattyry\\\\documents\\\\AzureML\\\\NextAgenda_CodeStory\\\\NextAgenda_CodeStory')\n",
    "\n",
    "with open('biotechcleaned.pkl', 'rb') as f:\n",
    "    data = pickle.load(f, encoding='utf8')\n",
    "    print(\"Data unpickled\")\n",
    "data = pd.DataFrame(data)\n",
    "thedata = data\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "#################################\n",
    "#If necessary, convert categories\n",
    "#################################\n",
    "#thedata['ReturnQuantile'] = thedata['ReturnQuantile'].map({0:0,1:1,2:1,3:1,4:2})\n",
    "print('Review the unique labels',thedata['Return3Bin_4Weeks'].unique())\n",
    "\n",
    "##########################################\n",
    "# clean up the text in the data with regex\n",
    "##########################################\n",
    "#Most clean up already done in pre-processing script in a jupyter notebook.\n",
    "thedata['fulltext'] = thedata['fulltext'].str.encode('utf-8')\n",
    "thedata['fulltext'] = thedata['fulltext'].str.lower()\n",
    "\n",
    "def clean_text(row):\n",
    "    text = str(row['fulltext'])\n",
    "\n",
    "    # Remove newline characters\n",
    "    cleantext = text.replace('\\r\\n', ' ')\n",
    "\n",
    "    # Convert HTML punctuation chaaracters\n",
    "    cleantext = cleantext.replace('.', '')\n",
    "\n",
    "    #remove non alpha characters and specific noise\n",
    "    #cleantext = re.sub(r'\\d+', '',cleantext)\n",
    "    cleantext = re.sub(r'^b','',cleantext)\n",
    "    cleantext = re.sub(r'[^\\w]',' ',cleantext)\n",
    "\n",
    "    #remove specific noise\n",
    "    cleantext = cleantext.translate(str.maketrans({'‘':' ','’':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({',':' ',',':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'\"':' ','%':' '}))\n",
    "\n",
    "    #remove punctuation\n",
    "    punctpattern = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    cleanttext = re.sub(punctpattern,'', cleantext)\n",
    "\n",
    "    #remove single letter word\n",
    "    cleantext = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', cleantext) \n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleantext = re.sub('\\s+', ' ', cleantext).strip()\n",
    "\n",
    "    return cleantext\n",
    "\n",
    "#apply regex fixes to the input text column\n",
    "thedata['CleanText'] = thedata.apply(clean_text, axis=1)\n",
    "justcleandocs=thedata.drop(['fulltext'], axis=1)\n",
    "\n",
    "#save a cleaned copy to inspect\n",
    "justcleandocs.to_csv('C:\\\\glove\\cleaneddata2.tsv', sep='\\t', encoding='utf-8')\n",
    "print(justcleandocs.shape)\n",
    "justcleandocs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post regex justcleandocs 131    acad overview we are biopharmaceutical company...\n",
      "133    acad company overview we are biopharmaceutical...\n",
      "134    acad company overview we are biopharmaceutical...\n",
      "135    acad company overview we are biopharmaceutical...\n",
      "136    acad overview we are biopharmaceutical company...\n",
      "137    acad overview we are biopharmaceutical company...\n",
      "138    acad overview we are biopharmaceutical company...\n",
      "139    acad overview we are biopharmaceutical company...\n",
      "140    acad overview we are biopharmaceutical company...\n",
      "181    achn overview we are biopharmaceutical company...\n",
      "Name: CleanText, dtype: object\n",
      "head of just labels     Return3Bin_4Weeks\n",
      "131                 0\n",
      "133                 2\n",
      "134                 0\n",
      "135                 0\n",
      "136                 2\n",
      "    Return3Bin_4Weeks\n",
      "131                 0\n",
      "133                 2\n",
      "134                 0\n",
      "135                 0\n",
      "136                 2\n",
      "      Return3Bin_4Weeks\n",
      "65850                 0\n",
      "65851                 1\n",
      "65852                 0\n",
      "65853                 0\n",
      "65854                 2\n",
      "[0, 2, 1]\n",
      "Categories (3, int64): [0 < 1 < 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "################################\n",
    "# Convert labels to categorical\n",
    "################################\n",
    "\n",
    "justcleandocs=thedata.drop(['fulltext','Return3Bin_4Weeks'], axis=1)\n",
    "justcleandocs = justcleandocs['CleanText']\n",
    "print('post regex justcleandocs',justcleandocs.head(10))\n",
    "\n",
    "justlabels=thedata.drop(['fulltext','CleanText'], axis=1)\n",
    "justlabels=pd.DataFrame(justlabels['Return3Bin_4Weeks'])\n",
    "print('head of just labels',justlabels.head(5))\n",
    "print(justlabels.head())\n",
    "print(justlabels.tail())\n",
    "print(justlabels['Return3Bin_4Weeks'].unique())\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Set Global Vars\n",
    "####################################################\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = .00011\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT_RATE = 0.45\n",
    "INNERLAYER_DROPOUT_RATE = 0.15\n",
    "np.random.seed(2032)\n",
    "\n",
    "#change directory to write results\n",
    "os.chdir('C:\\\\')\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = BASE_DIR + '/glove/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_index {0: 0, 1: 1, 2: 2}\n",
      "Found 23887 unique tokens\n",
      "Shape of data tensor:  (916, 10000)\n",
      "Shape of label tensor:  (916, 3)\n",
      "length of y_val 183\n",
      "shape of y_val (183, 3)\n",
      "length of X_val 183\n",
      "shape of X_val (183, 10000)\n",
      "test and training sets saved to disk for later evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################\n",
    "# Format our text samples and labels for use in Keras\n",
    "######################################################\n",
    "# Then we can format our text samples and labels into tensors that can be fed into a neural network. \n",
    "# Here we tokenize our source 'justcleandocs'\n",
    "# note that the values here are ultimately indexes to the actual words\n",
    "\n",
    "#convert text format\n",
    "justcleandocslist  = justcleandocs.values\n",
    "justcleandocslist[6]\n",
    "labels  = justlabels.values\n",
    "labels_index = {}\n",
    "#labels_index =  {0:0,1:1,2:2,3:3,4:4}\n",
    "labels_index =  {0:0,1:1,2:2}\n",
    "print('labels_index', labels_index)\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(justcleandocslist) #tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(justcleandocslist) #sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index #word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "#print('sequences first', sequences[0])\n",
    "\n",
    "#Pad sequences so that they all have the same length in a batch of input data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', truncating='pre')\n",
    "sequences = None\n",
    "texts = None\n",
    "\n",
    "\n",
    "##################################################\n",
    "#build label array from target y label in data set\n",
    "##################################################\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor: ', data.shape)\n",
    "print('Shape of label tensor: ', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('length of y_val',len(y_val))\n",
    "print('shape of y_val',y_val.shape)\n",
    "print('length of X_val',len(X_val))\n",
    "print('shape of X_val',X_val.shape)\n",
    "\n",
    "os.chdir('C:\\\\glove\\\\nextagenda')\n",
    "\n",
    "#from itertools import islice\n",
    "#head = list(islice(y_val, 6))\n",
    "#print('head of yval',head)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Save Validation Set for Evaluation\n",
    "####################################\n",
    "np.savetxt('y_val_3bin.txt', y_val, delimiter=',')\n",
    "np.savetxt('X_val_3bin.txt', X_val,  fmt='%s', delimiter=',')\n",
    "print('test and training sets saved to disk for later evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors to prepare the embedding layer...\n",
      "C:\\glove\\nextagenda\n",
      "Loading Glove Model...\n",
      "   0         1         2         3         4         5         6         7    \\\n",
      "0  the  0.046560  0.213180 -0.007436 -0.458540 -0.035639  0.236430 -0.288360   \n",
      "1    , -0.255390 -0.257230  0.131690 -0.042688  0.218170 -0.022702 -0.178540   \n",
      "2    . -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
      "3   of -0.076947 -0.021211  0.212710 -0.722320 -0.139880 -0.122340 -0.175210   \n",
      "4   to -0.257560 -0.057132 -0.671900 -0.380820 -0.364210 -0.082155 -0.010955   \n",
      "\n",
      "        8         9      ...          291       292       293       294  \\\n",
      "0  0.215210 -0.134860    ...    -0.013064 -0.296860 -0.079913  0.195000   \n",
      "1  0.107560  0.058936    ...     0.075968 -0.014359 -0.073794  0.221760   \n",
      "2  0.236970  0.328700    ...     0.060148 -0.156190 -0.119490  0.234450   \n",
      "3  0.121370 -0.070866    ...    -0.366730 -0.386030  0.302900  0.015747   \n",
      "4 -0.082047  0.460560    ...    -0.012806 -0.597070  0.317340 -0.252670   \n",
      "\n",
      "        295       296       297       298       299       300  \n",
      "0  0.031549  0.285060 -0.087461  0.009061 -0.209890  0.053913  \n",
      "1  0.146520  0.566860  0.053307 -0.232900 -0.122260  0.354990  \n",
      "2  0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
      "3  0.340360  0.478410  0.068617  0.183510 -0.291830 -0.046533  \n",
      "4  0.543840  0.063007 -0.049795 -0.160430  0.046744 -0.070621  \n",
      "\n",
      "[5 rows x 301 columns]\n",
      "shape of glove model (400000, 301)\n",
      "wordkeys type of file <class 'pandas.core.series.Series'>\n",
      "Found 400000 word vectors.\n",
      "Building Embedding Matrix...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# Preparing the embedding layer\n",
    "########################################\n",
    "\n",
    "#load in word vectors from glove reference global English data set\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# see more reference links at bottom\n",
    "\n",
    "print('Loading word vectors to prepare the embedding layer...')\n",
    "print(os.getcwd())\n",
    "\n",
    "embeddings_index = {}\n",
    "print('Loading Glove Model...')\n",
    "gloveFile = 'C:\\\\glove\\\\glove6B300d.txt'\n",
    "words = pd.read_table(gloveFile, sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(words.head(5))\n",
    "print('shape of glove model',words.shape)\n",
    "\n",
    "wordkeys=words.iloc[:,0]\n",
    "print('wordkeys type of file', type(wordkeys))\n",
    "words2 = words.rename(columns={ words.columns[0]: \"words\" })\n",
    "words2['words'].apply(str)\n",
    "#print(words2.dtypes)\n",
    "\n",
    "embeddings_index = words2.set_index('words').T.to_dict('list')\n",
    "\n",
    "#print(dict(list(embeddings_index.items())[0:2]))\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "#usage of pandas function dataFrame.to_dict(outtype='dict') outtype : str {‘dict’, ‘list’, ‘series’}\n",
    "\n",
    "\n",
    "#################################\n",
    "#Build the embedding matrix\n",
    "#################################\n",
    "\n",
    "print('Building Embedding Matrix...')\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1D Convnet with global maxpooling\n",
      "Shape of training data sample tensor:  (733, 10000)\n",
      "Shape of training label tensor:  (733, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################\n",
    "#Training a 1D convnet\n",
    "##############################################\n",
    "\n",
    "print('Train 1D Convnet with global maxpooling')\n",
    "print('Shape of training data sample tensor: ', X_train.shape)\n",
    "print('Shape of training label tensor: ', y_train.shape)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(INNERLAYER_DROPOUT_RATE)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(INNERLAYER_DROPOUT_RATE)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='elu', kernel_initializer='lecun_uniform')(x) # best initializers: #glorot_normal #VarianceScaling #lecun_uniform\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "preds = Dense(len(labels_index), activation='softmax')(x) #no initialization in output layer\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 733 samples, validate on 183 samples\n",
      "Epoch 1/24\n",
      "733/733 [==============================] - 330s - loss: 1.2609 - acc: 0.3302 - val_loss: 1.1046 - val_acc: 0.3497\n",
      "Epoch 2/24\n",
      "733/733 [==============================] - 293s - loss: 1.1536 - acc: 0.3834 - val_loss: 1.0895 - val_acc: 0.4153\n",
      "Epoch 3/24\n",
      "733/733 [==============================] - 288s - loss: 1.1175 - acc: 0.3874 - val_loss: 1.0802 - val_acc: 0.3607\n",
      "Epoch 4/24\n",
      "733/733 [==============================] - 324s - loss: 1.0857 - acc: 0.4120 - val_loss: 1.0744 - val_acc: 0.3825\n",
      "Epoch 5/24\n",
      "733/733 [==============================] - 331s - loss: 1.0675 - acc: 0.4256 - val_loss: 1.0710 - val_acc: 0.3934\n",
      "Epoch 6/24\n",
      "733/733 [==============================] - 338s - loss: 1.0693 - acc: 0.4338 - val_loss: 1.0857 - val_acc: 0.4153\n",
      "Epoch 7/24\n",
      "733/733 [==============================] - 331s - loss: 1.0571 - acc: 0.4447 - val_loss: 1.0910 - val_acc: 0.3825\n",
      "Epoch 8/24\n",
      "733/733 [==============================] - 320s - loss: 1.0301 - acc: 0.4666 - val_loss: 1.0686 - val_acc: 0.3825\n",
      "Epoch 9/24\n",
      "733/733 [==============================] - 314s - loss: 1.0330 - acc: 0.4638 - val_loss: 1.1050 - val_acc: 0.3552\n",
      "Epoch 10/24\n",
      "733/733 [==============================] - 313s - loss: 0.9989 - acc: 0.4980 - val_loss: 1.0557 - val_acc: 0.4153\n",
      "Epoch 11/24\n",
      "733/733 [==============================] - 334s - loss: 0.9991 - acc: 0.4925 - val_loss: 1.0921 - val_acc: 0.4153\n",
      "Epoch 12/24\n",
      "192/733 [======>.......................] - ETA: 214s - loss: 0.9385 - acc: 0.5469"
     ]
    }
   ],
   "source": [
    "################################\n",
    "#Compile model, set optimizers\n",
    "################################ \n",
    "\n",
    "adam = optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=0.5)#, clipnorm=1.)\n",
    "rmsprop = optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=1e-08, decay=0.00)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=24,\n",
    "          validation_data=(X_val, y_val), callbacks=[early_stopping, history])\n",
    "\n",
    "\n",
    "##############################\n",
    "# Save Model and Plots\n",
    "##############################\n",
    "model.save('C:\\\\glove\\\\StockText_3Level3EvenClass_modelNov2_8pm.h5')\n",
    " \n",
    "import matplotlib.pyplot as plt  \n",
    "plt.figure(1)  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()  \n",
    "\n",
    "#plot_model(model, to_file='C:\\\\glove\\stocktext_model3class.png')\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "##############################\n",
    "# More helpful links\n",
    "##############################\n",
    "\n",
    "#We can also test how well we would have performed by not using pre-trained word embeddings, \n",
    "#but instead initializing our Embedding layer from scratch and learning its weights during training. \n",
    "\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#https://arxiv.org/abs/1603.03827 \n",
    "#https://nlp.stanford.edu/projects/glove/ \n",
    "#https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "#https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim?rq=1\n",
    "#https://stackoverflow.com/questions/14415741/numpy-array-vs-asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
